import os
import time
from datetime import datetime, timedelta
from pathlib import Path
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_json, struct, col

# -----------------------
# –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø
# -----------------------

# --- –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è Kafka ---
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")
KAFKA_OUTPUT_TOPIC = "athlete_event_results"
KAFKA_PACKAGE = "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1"

# --- –û–±–ª—ñ–∫–æ–≤—ñ –¥–∞–Ω—ñ —Ç–∞ JDBC URL ---
jdbc_host = "217.61.57.46"
jdbc_port = "3306"
jdbc_db = "olympic_dataset"
jdbc_user = "neo_data_admin"
jdbc_password = "Proyahaxuqithab9oplp"
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –ø–æ–≤–Ω–∏–π JDBC URL
jdbc_url = f"jdbc:mysql://{jdbc_host}:{jdbc_port}/{jdbc_db}?useSSL=false&serverTimezone=UTC&allowPublicKeyRetrieval=true"
jdbc_table_events = "athlete_event_results"


try:
    script_dir = Path(__file__).resolve().parent
    project_root = script_dir.parent.parent
    mysql_jar_path = str(project_root / "jars" / "mysql-connector-j-8.0.32.jar")
except NameError:
    print(
        "Warning: Could not determine project root path automatically. Using default."
    )
    mysql_jar_path = r"C:\Project\MasterSc\Date_Engineering\Final_Project\Part_1\jars\mysql-connector-j-8.0.32.jar"


# ‚úÖ –§–ê–ô–õ –î–õ–Ø –ó–ë–ï–†–Ü–ì–ê–ù–ù–Ø –û–°–¢–ê–ù–ù–¨–û–ì–û ID
STATE_FILE = str(Path(__file__).resolve().parent / "producer_state.txt")
POLL_INTERVAL_SECONDS = 60
BATCH_SIZE = 100000
INCREMENTAL_COLUMN = "athlete_id"


def get_last_processed_id():
    """–ß–∏—Ç–∞—î –æ—Å—Ç–∞–Ω–Ω—ñ–π –æ–±—Ä–æ–±–ª–µ–Ω–∏–π ID –∑ —Ñ–∞–π–ª—É —Å—Ç–∞–Ω—É."""
    try:
        with open(STATE_FILE, "r") as f:
            return int(f.read().strip())
    except (FileNotFoundError, ValueError):
        return 0  # –ü–æ—á–∞—Ç–∏ –∑ 0


def update_last_processed_id(max_id):
    """–û–Ω–æ–≤–ª—é—î –æ—Å—Ç–∞–Ω–Ω—ñ–π –æ–±—Ä–æ–±–ª–µ–Ω–∏–π ID —É —Ñ–∞–π–ª—ñ —Å—Ç–∞–Ω—É."""
    with open(STATE_FILE, "w") as f:
        f.write(str(max_id))


def run_iterative_producer():
    # 1. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è SparkSession
    spark = (
        SparkSession.builder.appName("IterativeMySQLKafkaProducer")
        .config("spark.jars", mysql_jar_path)
        .config("spark.jars.packages", KAFKA_PACKAGE)
        .master("local[*]")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("WARN")
    print("SparkSession created for Iterative Producer.")
    print(f"Using MySQL Connector JAR path: {mysql_jar_path}")

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ JAR-—Ñ–∞–π–ª—É
    if not Path(mysql_jar_path).is_file():
        raise FileNotFoundError(f"MySQL JDBC JAR not found at: {mysql_jar_path}")

    while True:
        start_time = datetime.now()
        last_id = get_last_processed_id()
        current_max_id = 0  # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –¥–ª—è –ª–æ–≥—ñ–∫–∏ –æ–±—Ä–æ–±–∫–∏ —Ü–∏–∫–ª—É

        if last_id == 0:
            print(
                "\n--- Starting NEW CYCLE: Reading from the beginning of the dataset. ---"
            )

        print(
            f"\n--- Starting Poll ({start_time.strftime('%H:%M:%S')}): Reading next {BATCH_SIZE} records where {INCREMENTAL_COLUMN} > {last_id} ---"
        )

        # 2. –§–æ—Ä–º—É–≤–∞–Ω–Ω—è —ñ–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø–∏—Ç—É –∑ LIMIT –¥–ª—è Batching
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ SELECT * –∑ ORDER BY —Ç–∞ LIMIT –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –ø—ñ–¥–∑–∞–ø–∏—Ç—É
        dbtable_query = f"(SELECT * FROM {jdbc_table_events} WHERE {INCREMENTAL_COLUMN} > {last_id} ORDER BY {INCREMENTAL_COLUMN} ASC LIMIT {BATCH_SIZE}) AS new_records"

        try:
            new_events_df = (
                spark.read.format("jdbc")
                .option("url", jdbc_url)
                .option("driver", "com.mysql.cj.jdbc.Driver")
                .option("dbtable", dbtable_query)
                .option("user", jdbc_user)
                .option("password", jdbc_password)
                .load()
            )

            record_count = new_events_df.count()

            if record_count > 0:
                print(f"Found {record_count} new records. Processing...")

                # 3. –û–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞–Ω—É: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ max() –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ü–∏–∫–ª—É
                max_current_id_row = new_events_df.agg(
                    {INCREMENTAL_COLUMN: "max"}
                ).collect()[0]
                current_max_id = max_current_id_row[f"max({INCREMENTAL_COLUMN})"]

                # –ü–µ—Ä–µ–∫–æ–Ω–∞—î–º–æ—Å—å, —â–æ –º–∏ —Ä–æ–±–∏–º–æ —ñ–Ω–∫—Ä–µ–º–µ–Ω—Ç –ª–∏—à–µ –≤–ø–µ—Ä–µ–¥
                if current_max_id > last_id:
                    update_last_processed_id(current_max_id)
                    print(f"State updated: max_id_processed is now {current_max_id}")
                else:
                    print(
                        f"Warning: Max ID found ({current_max_id}) is not greater than last ID ({last_id}). Not updating state."
                    )

                # 4. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è Kafka (–≤–∫–ª—é—á–Ω–æ –∑ –∫–ª—é—á–æ–≤–∏–º–∏ –ø–æ–ª—è–º–∏)
                kafka_df = new_events_df.select(
                    col("athlete_id").cast("string").alias("key"),
                    to_json(
                        struct(
                            col("athlete_id"),
                            col("sport"),
                            col("event"),
                            col("medal"),
                            col("country_noc"),
                        )
                    ).alias("value"),
                )

                # 5. –ó–∞–ø–∏—Å —É Kafka-—Ç–æ–ø—ñ–∫ (BATCH)
                (
                    kafka_df.write.format("kafka")
                    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS)
                    .option("topic", KAFKA_OUTPUT_TOPIC)
                    .mode("append")
                    .save()
                )
                print(f"Successfully published {record_count} records to Kafka.")

                # 6. –õ–û–ì–Ü–ö–ê –ó–ê–ü–£–°–ö–£ –¶–ò–ö–õ–£ (LOOPING):
                # –Ø–∫—â–æ –º–∏ –ø—Ä–æ—á–∏—Ç–∞–ª–∏ –º–µ–Ω—à–µ, –Ω—ñ–∂ BATCH_SIZE, —Ü–µ –æ–∑–Ω–∞—á–∞—î, —â–æ –º–∏ –¥–æ—Å—è–≥–ª–∏ –∫—ñ–Ω—Ü—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö.
                if record_count < BATCH_SIZE:
                    print(
                        f"\n--- END OF DATASET REACHED. Resetting state to 0 for next iteration. ---"
                    )
                    update_last_processed_id(0)  # –°–∫–∏–¥–∞—î–º–æ —Å—Ç–∞–Ω –Ω–∞ 0

            else:
                # 7. –û–±—Ä–æ–±–∫–∞ –ø–æ—Ä–æ–∂–Ω—å–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
                if last_id > 0 and current_max_id == last_id:
                    # –Ø–∫—â–æ –º–∏ —Ä–∞–Ω—ñ—à–µ —á–∏—Ç–∞–ª–∏ —ñ —Ç–µ–ø–µ—Ä –Ω—ñ—á–æ–≥–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏, —Ü–µ –∫—ñ–Ω–µ—Ü—å
                    print(
                        f"\n--- FULL DATASET PROCESSED. Resetting state to 0 for next iteration. ---"
                    )
                    update_last_processed_id(0)  # –°–∫–∏–¥–∞—î–º–æ —Å—Ç–∞–Ω –Ω–∞ 0
                else:
                    print("No new records found. Waiting...")

        except Exception as e:
            print(f"\nüõë An unexpected error occurred during polling:")
            print(f"Error: {e}")
            if "ClassNotFoundException" in str(e):
                print(f"üö® Check the path to your MySQL JAR file: {mysql_jar_path}")

        # 8. –ß–ê–° –ù–ê–°–¢–£–ü–ù–û–ì–û –û–ù–û–í–õ–ï–ù–ù–Ø
        end_time = datetime.now()

        # –û–±—á–∏—Å–ª—é—î–º–æ —Ñ–∞–∫—Ç–∏—á–Ω–∏–π —á–∞—Å, —â–æ –∑–∞–ª–∏—à–∏–≤—Å—è
        time_elapsed = (end_time - start_time).total_seconds()
        sleep_duration = max(0, POLL_INTERVAL_SECONDS - time_elapsed)

        next_poll_time = end_time + timedelta(seconds=sleep_duration)

        print(f"\nSleeping for {round(sleep_duration)} seconds...")
        print(f"NEXT POLL SCHEDULED FOR: {next_poll_time.strftime('%H:%M:%S')}")

        time.sleep(sleep_duration)


if __name__ == "__main__":
    try:
        run_iterative_producer()
    except KeyboardInterrupt:
        print("\nProducer stopped by user (Ctrl+C).")
    finally:
        try:
            spark = SparkSession.builder.appName("TempStopper").getOrCreate()
            spark.stop()
        except Exception:
            pass
